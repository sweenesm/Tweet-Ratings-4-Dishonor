{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UacEbgrnGL5Z"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caj1Qb9OCC_r"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-sVEPTWVDak"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNgOvk0rVEh5"
   },
   "outputs": [],
   "source": [
    "!pip install pytesseract==0.3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntG49SIsXweU"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8ikCjcaXwW5"
   },
   "outputs": [],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3hOCq7wVEb7"
   },
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ8jel0TVEYC"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qDDHI3_Yo8G"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqVFrf4LhPGs"
   },
   "outputs": [],
   "source": [
    "!pip install youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qstox0qMms9c"
   },
   "outputs": [],
   "source": [
    "!pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZZamtVR3yKD"
   },
   "outputs": [],
   "source": [
    "!pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 21655,
     "status": "ok",
     "timestamp": 1682107880846,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "XOJlR6swdDth"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import h5py\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"wordnet\")"
   ],
   "metadata": {
    "id": "f7hknJgoYVLD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 787,
     "status": "ok",
     "timestamp": 1682107882024,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "rKwlA5dDmt1H"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from datetime import timezone, datetime, timedelta\n",
    "from dateutil import tz\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import kaleido\n",
    "from deep_translator import GoogleTranslator\n",
    "from pynvml import *\n",
    "import math\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import ConvBertModel, ConvBertTokenizer, TFConvBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1682107884004,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "AjZZLIcDpQLQ"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import *\n",
    "import wget\n",
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "import whisper\n",
    "import time\n",
    "import librosa\n",
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhZ3CwFPYB1w"
   },
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small.en\")  # load the small model for transcribing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 5618,
     "status": "ok",
     "timestamp": 1682107923942,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "hwdRJ7VvYplQ"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1682108216524,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "GcV6-s3oC8sA"
   },
   "outputs": [],
   "source": [
    "# API key:\n",
    "consumer_key = \"blah\"\n",
    "# API key secret:\n",
    "consumer_secret = \"blah\"\n",
    "# Access token:\n",
    "access_token = \"blah\"\n",
    "# Access token secret:\n",
    "access_token_secret = \"blah\"\n",
    "\n",
    "# For Twitter API v1.1:\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1682107924136,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "FBj53Zcujhom"
   },
   "outputs": [],
   "source": [
    "end_date_utc = datetime(\n",
    "    year=2023, month=3, day=4, hour=18, minute=59, second=59, tzinfo=timezone.utc\n",
    ")\n",
    "delta = timedelta(days=-60)\n",
    "start_date_utc = end_date_utc + delta\n",
    "end_date = end_date_utc.strftime(\"%m-%d-%Y\")\n",
    "start_date = start_date_utc.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mlsCcN9Elwm"
   },
   "outputs": [],
   "source": [
    "# To load ~9 politicians' accounts to analyze:\n",
    "day_num = 1\n",
    "accounts_df = pd.read_csv(\n",
    "    \"/content/drive/MyDrive/Colab_Notebooks/TwitterAccountList.csv\"\n",
    ")\n",
    "polit_list_old = []\n",
    "party_list = []\n",
    "state_list = []\n",
    "name_list = []\n",
    "user_list_old = []\n",
    "for i in range(len(accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"Usernames\"])):\n",
    "    user_list_old.append(\n",
    "        accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"Usernames\"].iloc[i]\n",
    "    )\n",
    "    party_list.append(\n",
    "        accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"Party\"].iloc[i]\n",
    "    )\n",
    "    state_list.append(\n",
    "        accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"State\"].iloc[i]\n",
    "    )\n",
    "    name_list.append(\n",
    "        accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"Politician\"].iloc[i]\n",
    "    )\n",
    "    polit_list_old.append(\n",
    "        str(accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"AccountID_1\"].iloc[i])\n",
    "        + str(\n",
    "            accounts_df.loc[accounts_df[\"Grouping\"] == day_num, \"AccountID_2\"].iloc[i]\n",
    "        )\n",
    "    )\n",
    "polit_list_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682107924486,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "eA2LIyCyiH-f"
   },
   "outputs": [],
   "source": [
    "# Optical Character Recognition (OCR) of text in photos\n",
    "def process_pics(url, file_id):\n",
    "    try:\n",
    "        wget.download(url, out=file_id)\n",
    "        img = cv2.imread(file_id)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        os.remove(file_id)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"process_pics exception {url}\")\n",
    "        text = \"\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682107924488,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "3ACKBTwziHvV"
   },
   "outputs": [],
   "source": [
    "# Use regex to replace \\n with a space for tesseract OCR'ed text\n",
    "def process_tess(input_text):\n",
    "    pat_n1 = re.compile(\"(\\\\n)\")\n",
    "    pat_n2 = re.compile(\"(\\\\x0c)\")\n",
    "    text1 = re.sub(pat_n1, \" \", input_text)\n",
    "    return re.sub(pat_n2, \"\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682107924488,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "Hv75QwmUiHlO"
   },
   "outputs": [],
   "source": [
    "# Transcribe from the audio of videos\n",
    "def process_vids(url, file_id):\n",
    "    ydl_opts = {\"outtmpl\": file_id, \"quiet\": True, \"get-duration\": True}\n",
    "    try:\n",
    "        # Adapted from https://www.bogotobogo.com/VideoStreaming/YouTube/youtube-dl-embedding.php\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        # Load the audio with 16 kHz sampling rate\n",
    "        y, sr = librosa.load(file_id, sr=16000)\n",
    "        result = model.transcribe(y)\n",
    "        text = result[\"text\"].strip()\n",
    "        os.remove(file_id)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        print(type(e))\n",
    "        print(f\"process_vids exception {url} file {file_id}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Updated with suggestions from ChatGPT\n",
    "def transcr_media(df, jnum):\n",
    "    path_id = \"/content/drive/MyDrive/Colab_Notebooks/\"\n",
    "    pat_png = re.compile(\"\\W(png)\")\n",
    "    transcr_vids = [[] for i in range(len(df))]\n",
    "    transcr_pics = [[] for i in range(len(df))]\n",
    "    transcr_vids_em = [[] for i in range(len(df))]\n",
    "    transcr_pics_em = [[] for i in range(len(df))]\n",
    "    for i in range(len(df)):\n",
    "        if df.Videos[i] != []:\n",
    "            for kk in range(len(df.Videos[i])):\n",
    "                url = df.Videos[i][kk]\n",
    "                file_id = path_id + jnum + \"Tweet\" + str(i) + str(kk) + \".mp4\"\n",
    "                transcr_vids[i].append(process_vids(url, file_id))\n",
    "        if df.Pictures[i] != []:\n",
    "            for kk in range(len(df.Pictures[i])):\n",
    "                url = df.Pictures[i][kk]\n",
    "                if bool(re.findall(pat_png, str(url))):\n",
    "                    file_id = path_id + jnum + \"Tweet\" + str(i) + str(kk) + \".png\"\n",
    "                else:\n",
    "                    file_id = path_id + jnum + \"Tweet\" + str(i) + str(kk) + \".jpg\"\n",
    "                transcr_pics[i].append(process_pics(url, file_id))\n",
    "        if df.EmbedVideos[i] != []:\n",
    "            for kk in range(len(df.Videos[i])):\n",
    "                url = df.EmbedVideos[i][kk]\n",
    "                file_id = path_id + jnum + \"Embed\" + str(i) + str(kk) + \".mp4\"\n",
    "                transcr_vids_em[i].append(process_vids(url, file_id))\n",
    "        if df.EmbedPictures[i] != []:\n",
    "            for kk in range(len(df.EmbedPictures[i])):\n",
    "                url = df.EmbedPictures[i][kk]\n",
    "                if bool(re.findall(pat_png, str(url))):\n",
    "                    file_id = path_id + jnum + \"Embed\" + str(i) + str(kk) + \".png\"\n",
    "                else:\n",
    "                    file_id = path_id + jnum + \"Embed\" + str(i) + str(kk) + \".jpg\"\n",
    "                transcr_pics_em[i].append(process_pics(url, file_id))\n",
    "    df[\"Transcr_vids\"] = transcr_vids\n",
    "    df[\"Transcr_pics\"] = transcr_pics\n",
    "    df[\"Transcr_vids_em\"] = transcr_vids_em\n",
    "    df[\"Transcr_pics_em\"] = transcr_pics_em\n",
    "    return df"
   ],
   "metadata": {
    "id": "oNvhjfydIK9p",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682107924489,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682107924490,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "Ea9pmyuuFK4u"
   },
   "outputs": [],
   "source": [
    "# Adapted from ChatGPT\n",
    "# Assume average word length is 8 characters to stay below translate character limit of 5000\n",
    "def batch_translate(text, batch_len=600):\n",
    "    pat_text = re.compile(r\"[A-Za-z]{1,50}\")\n",
    "    if not re.findall(pat_text, text):\n",
    "        translated_text = \"\"\n",
    "    else:\n",
    "        # Split the text into batches of words\n",
    "        word_batches = []\n",
    "        wordies = text.split()\n",
    "        num_batches = math.ceil(len(wordies) / batch_len)\n",
    "        for i in range(num_batches):\n",
    "            word_batch = \"\"\n",
    "            start_index = i * batch_len\n",
    "            end_index = (i + 1) * batch_len\n",
    "            if end_index > len(wordies):\n",
    "                end_index = len(wordies)\n",
    "            for im in range(start_index, end_index):\n",
    "                word_batch += wordies[im] + \" \"\n",
    "            word_batches.append(word_batch)\n",
    "\n",
    "        # Translate each batch and combine the results\n",
    "        translated_text = \"\"\n",
    "        for batch in word_batches:\n",
    "            translated_batch = GoogleTranslator(source=\"auto\", target=\"en\").translate(\n",
    "                batch\n",
    "            )\n",
    "            translated_text += translated_batch + \" \"\n",
    "\n",
    "    return translated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682107924756,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "zKYqkOfQkYtW"
   },
   "outputs": [],
   "source": [
    "# Use regex to clean up some of the transcriptions; translate to English\n",
    "# Adapted from: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n",
    "def transcr_clean(df):\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-\\.]{2,50}\")\n",
    "    # Regex pattern for only alphanumeric, hyphenated text with 2 or more characters; removes punctuation other than period\n",
    "    df[\"PreCleanVids\"] = df[\"Transcr_vids\"].copy()\n",
    "    df[\"PreCleanPics\"] = df[\"Transcr_pics\"].copy()\n",
    "    for i in df.index:\n",
    "        for kk in range(len(df.Transcr_vids[i])):\n",
    "            df[\"PreCleanVids\"][i][kk] = \" \".join(\n",
    "                re.findall(pattern, batch_translate(str(df[\"Transcr_vids\"][i][kk])))\n",
    "            )\n",
    "        for kk in range(len(df.Transcr_pics[i])):\n",
    "            df[\"PreCleanPics\"][i][kk] = \" \".join(\n",
    "                re.findall(pattern, batch_translate(str(df[\"Transcr_pics\"][i][kk])))\n",
    "            )\n",
    "\n",
    "    df[\"PreCleanVids_em\"] = df[\"Transcr_vids_em\"].copy()\n",
    "    df[\"PreCleanPics_em\"] = df[\"Transcr_pics_em\"].copy()\n",
    "    for i in df.index:\n",
    "        for kk in range(len(df.Transcr_vids_em[i])):\n",
    "            df[\"PreCleanVids_em\"][i][kk] = \" \".join(\n",
    "                re.findall(pattern, batch_translate(str(df[\"Transcr_vids_em\"][i][kk])))\n",
    "            )\n",
    "        for kk in range(len(df.Transcr_pics_em[i])):\n",
    "            df[\"PreCleanPics_em\"][i][kk] = \" \".join(\n",
    "                re.findall(pattern, batch_translate(str(df[\"Transcr_pics_em\"][i][kk])))\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Made more efficient with ChatGPT\n",
    "def filter_non_words(df):\n",
    "    transcr_pics_clean, transcr_pics_clean_em = [], []\n",
    "    # Adapted from https://intellipaat.com/community/5638/removing-non-english-words-from-text-using-python\n",
    "    # Used both spacy and nltk lemmatizers because a word like \"governors\" was included this way\n",
    "    # whereas it wasn't when only the nltk lemmatizer was used\n",
    "    # splitting into batches of words make this faster because the spacy-identified entities list is shorter\n",
    "    # and don't have to test the whole list of words against them, only the batch of words\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    Lem = WordNetLemmatizer()\n",
    "    batch_len = 10\n",
    "    for i in df.index:\n",
    "        transcr_pics_clean.append([])\n",
    "        transcr_pics_clean_em.append([])\n",
    "        if df.PreCleanPics[i] != []:\n",
    "            for kj in range(len(df.PreCleanPics[i])):\n",
    "                clean_pics = \"\"\n",
    "                # Split the text into batches\n",
    "                word_batches = []\n",
    "                wordies = str(df.PreCleanPics[i][kj]).split()\n",
    "                num_batches = math.ceil(len(wordies) / batch_len)\n",
    "                for imi in range(num_batches):\n",
    "                    word_batch = \"\"\n",
    "                    start_index = imi * batch_len\n",
    "                    end_index = (imi + 1) * batch_len\n",
    "                    if end_index > len(wordies):\n",
    "                        end_index = len(wordies)\n",
    "                    for im in range(start_index, end_index):\n",
    "                        word_batch += wordies[im] + \" \"\n",
    "                    word_batches.append(word_batch)\n",
    "                for batch in word_batches:\n",
    "                    doc = nlp(batch.lower())\n",
    "                    for token in doc:\n",
    "                        lemword = Lem.lemmatize(token.text)\n",
    "                        if (\n",
    "                            lemword.lower() in words\n",
    "                            or token.lemma_ in words\n",
    "                            or not lemword.isalpha()\n",
    "                        ):\n",
    "                            if token.text == \".\":\n",
    "                                clean_pics = clean_pics.strip()\n",
    "                            clean_pics += token.text.lower() + \" \"\n",
    "                            continue\n",
    "                        else:\n",
    "                            for ent in doc.ents:\n",
    "                                doc_ent = nlp(ent.text)\n",
    "                                for tok_ent in doc_ent:\n",
    "                                    if lemword.lower() == tok_ent.text:\n",
    "                                        clean_pics += token.text.lower() + \" \"\n",
    "                                        break\n",
    "                transcr_pics_clean[i].append(clean_pics.strip())\n",
    "        if df.PreCleanPics_em[i] != []:\n",
    "            for kj in range(len(df.PreCleanPics_em[i])):\n",
    "                clean_pics_em = \"\"\n",
    "                # Split the text into batches\n",
    "                word_batches = []\n",
    "                wordies = str(df.PreCleanPics_em[i][kj]).split()\n",
    "                num_batches = math.ceil(len(wordies) / batch_len)\n",
    "                for imi in range(num_batches):\n",
    "                    word_batch = \"\"\n",
    "                    start_index = imi * batch_len\n",
    "                    end_index = (imi + 1) * batch_len\n",
    "                    if end_index > len(wordies):\n",
    "                        end_index = len(wordies)\n",
    "                    for im in range(start_index, end_index):\n",
    "                        word_batch += wordies[im] + \" \"\n",
    "                    word_batches.append(word_batch)\n",
    "                for batch in word_batches:\n",
    "                    doc_em = nlp(batch.lower())\n",
    "                    for token in doc_em:\n",
    "                        lemword = Lem.lemmatize(token.text)\n",
    "                        if (\n",
    "                            lemword.lower() in words\n",
    "                            or token.lemma_ in words\n",
    "                            or not lemword.isalpha()\n",
    "                        ):\n",
    "                            if token.text == \".\":\n",
    "                                clean_pics_em = clean_pics_em.strip()\n",
    "                            clean_pics_em += token.text.lower() + \" \"\n",
    "                            continue\n",
    "                        else:\n",
    "                            for ent in doc_em.ents:\n",
    "                                doc_ent_em = nlp(ent.text)\n",
    "                                for tok_ent in doc_ent_em:\n",
    "                                    if lemword.lower() == tok_ent.text:\n",
    "                                        clean_pics_em += token.text.lower() + \" \"\n",
    "                                        break\n",
    "                transcr_pics_clean_em[i].append(clean_pics_em.strip())\n",
    "    df[\"Clean_pics\"] = transcr_pics_clean\n",
    "    df[\"Clean_pics_em\"] = transcr_pics_clean_em\n",
    "    return df"
   ],
   "metadata": {
    "id": "xQ6rJM6HhmbA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682107924757,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     }
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682107924757,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "JuiHd3dLm4ch"
   },
   "outputs": [],
   "source": [
    "# If they're short (<140 characters), add media transcriptions to the text of the tweet,\n",
    "# otherwise add them to the text of \"SpeechToRate\"\n",
    "def sort_transcripts(df):\n",
    "    vid_count, pic_count = 0, 0\n",
    "    tweet_rate, speech_rate, embed_rate = [], [], []\n",
    "    for i in df.index:\n",
    "        tweet_rate.append(str(df.Translated[i]))\n",
    "        embed_rate.append(str(df.EmbedTrans[i]))\n",
    "        speech_rate.append(\"\")\n",
    "        for kk in range(len(df.Clean_pics[i])):\n",
    "            if str(df.Clean_pics[i][kk]) != \"nan\":\n",
    "                pic_count += 1\n",
    "                if len(str(df.Clean_pics[i][kk])) < 140:\n",
    "                    tweet_rate[i] += \" \" + str(df.Clean_pics[i][kk])\n",
    "                else:\n",
    "                    speech_rate[i] += str(df.Clean_pics[i][kk]) + \" \"\n",
    "        for kk in range(len(df.PreCleanVids[i])):\n",
    "            if str(df.PreCleanVids[i][kk]) != \"nan\":\n",
    "                vid_count += 1\n",
    "                if len(str(df.PreCleanVids[i][kk])) < 140:\n",
    "                    tweet_rate[i] += \" \" + str(df.PreCleanVids[i][kk])\n",
    "                else:\n",
    "                    speech_rate[i] += str(df.PreCleanVids[i][kk]) + \" \"\n",
    "        speech_rate[i].strip()\n",
    "\n",
    "        for kk in range(len(df.Clean_pics_em[i])):\n",
    "            if str(df.Clean_pics_em[i][kk]) != \"nan\":\n",
    "                pic_count += 1\n",
    "                if len(str(df.Clean_pics_em[i][kk])) < 140:\n",
    "                    embed_rate[i] += \" \" + str(df.Clean_pics_em[i][kk])\n",
    "                else:\n",
    "                    speech_rate[i] += str(df.Clean_pics_em[i][kk]) + \" \"\n",
    "        for kk in range(len(df.PreCleanVids_em[i])):\n",
    "            if str(df.PreCleanVids_em[i][kk]) != \"nan\":\n",
    "                vid_count += 1\n",
    "                if len(str(df.PreCleanVids_em[i][kk])) < 140:\n",
    "                    embed_rate[i] += \" \" + str(df.PreCleanVids_em[i][kk])\n",
    "                else:\n",
    "                    speech_rate[i] += str(df.PreCleanVids_em[i][kk]) + \" \"\n",
    "        speech_rate[i].strip()\n",
    "\n",
    "    df[\"EmbedToRate\"] = embed_rate\n",
    "    df[\"TweetToRate\"] = tweet_rate\n",
    "    df[\"SpeechToRate\"] = speech_rate\n",
    "    return df, pic_count, vid_count"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ChatGPT suggested improved version:\n",
    "def process_users(input_text: str) -> str:\n",
    "    pat_user = re.compile(\"@([A-Za-z\\d_])\\w+\")\n",
    "    if not input_text:\n",
    "        return \"\"\n",
    "    return pat_user.sub(\"name\", input_text)"
   ],
   "metadata": {
    "id": "599JZ4aqSAoK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682107924757,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     }
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ChatGPT-improved version\n",
    "# Expand select abbreviations (such as imo, RT, and lol), before expanding hashtags\n",
    "def process_abbrevs(input_text: str) -> str:\n",
    "    pat_abbrev_dict = {\n",
    "        re.compile(r\"\\bimo\\b\"): \"in my opinion\",\n",
    "        re.compile(r\"\\bRT\\b\"): \"retweet\",\n",
    "        re.compile(r\"\\bimho\\b\"): \"in my humble opinion\",\n",
    "        re.compile(r\"\\blol\\b\"): \"laugh out loud\",\n",
    "        re.compile(r\"\\blmao\\b\"): \"laugh my ass off\",\n",
    "        re.compile(r\"\\bgov't\\b\"): \"government\",\n",
    "        re.compile(r\"\\bBidenflation\\b\"): \"biden's inflation\",\n",
    "        re.compile(r\"\\bBidenomics\\b\"): \"biden's economic policies\",\n",
    "        re.compile(r\"\\bmaga\\b\"): \"make america great again\",\n",
    "        re.compile(r\"\\bACA\\b\"): \"affordable care act\",\n",
    "        re.compile(r\"\\bObamacare\\b\"): \"affordable care act\",\n",
    "        re.compile(r\"\\bImo\\b\"): \"in my opinion\",\n",
    "        re.compile(r\"\\bIMO\\b\"): \"in my opinion\",\n",
    "        re.compile(r\"\\bImho\\b\"): \"in my humble opinion\",\n",
    "        re.compile(r\"\\bIMHO\\b\"): \"in my humble opinion\",\n",
    "        re.compile(r\"\\bMAGA\\b\"): \"make america great again\",\n",
    "        re.compile(r\"\\bLol\\b\"): \"laugh out loud\",\n",
    "        re.compile(r\"\\bLOL\\b\"): \"laugh out loud\",\n",
    "        re.compile(r\"\\bLmao\\b\"): \"laugh my ass off\",\n",
    "        re.compile(r\"\\bLMAO\\b\"): \"laugh my ass off\",\n",
    "    }\n",
    "    if not input_text:\n",
    "        return \"\"\n",
    "    for i in pat_abbrev_dict.keys():\n",
    "        output_text = re.sub(i, pat_abbrev_dict[i], input_text)\n",
    "    return output_text"
   ],
   "metadata": {
    "id": "bcj4JLZuSiiv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682107924757,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     }
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682107924758,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "VTUINDKciY6l"
   },
   "outputs": [],
   "source": [
    "# Breaking up hashtags into individual words by capitalization, e.g. #NeedMoreMoney = \"need more money\"\n",
    "# Adapted from https://stackoverflow.com/questions/68448243/efficient-way-to-split-multi-word-hashtag-in-python\n",
    "def process_hashtags(input_text: str) -> str:\n",
    "    return re.sub(\n",
    "        r\"#[A-Za-z]\\S*\",\n",
    "        lambda m: \" \".join(\n",
    "            re.findall(\"[A-Z][^A-Z]*|[a-z][^A-Z]*\", m.group().lstrip(\"#\"))\n",
    "        ),\n",
    "        input_text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682107924758,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "T8ogy14xiYxu"
   },
   "outputs": [],
   "source": [
    "# Replace urls with \"weblink\" and make everything lowercase\n",
    "# Adapted from https://stackoverflow.com/questions/839994/extracting-a-url-in-python\n",
    "def process_urls(input_text: str) -> str:\n",
    "    pat_urls = re.compile(r\"(https?:\\/\\/\\S+\\b)\")\n",
    "    if not input_text:\n",
    "        return \"\"\n",
    "    output_text = re.sub(pat_urls, \"weblink\", input_text)\n",
    "    output_text = output_text.lower()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682107924758,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "6uhAdPg-J8Mk"
   },
   "outputs": [],
   "source": [
    "# Adapted from: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n",
    "# Regex pattern for tweet text: only alphanumeric, hyphenated text with 2 or more characters; removes other punctuation\n",
    "def cleaner(df):\n",
    "    pat_clean = re.compile(r\"[A-Za-z0-9\\-]{2,50}\")\n",
    "    df[\"cleaned_tweet\"] = df[\"preclean\"].str.findall(pat_clean).str.join(\" \")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682107924758,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "jV54LDQW4MLf"
   },
   "outputs": [],
   "source": [
    "# Function to break text into sentences using spacy\n",
    "def sentencize(text):\n",
    "    doc = nlp(text)\n",
    "    doc_sents = [str(sent) for sent in doc.sents]\n",
    "    return doc_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682107924758,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "pPcYyC_n4QzS"
   },
   "outputs": [],
   "source": [
    "# Function to make a list of all the sentences, in order\n",
    "def sentencelist(df):\n",
    "    df_sents_all = []\n",
    "    for i in df.index:\n",
    "        for jj in range(len(df.sentences[i])):\n",
    "            if str(df.sentences.at[i][jj]) != \"\":\n",
    "                df_sents_all.append(str(df.sentences.at[i][jj]))\n",
    "    return df_sents_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682107924758,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "ZuVuk34KEl0N"
   },
   "outputs": [],
   "source": [
    "# sentencelistize - break transcript into sentences and list in a dataframe\n",
    "def sentencelistize(df):\n",
    "    df[\"sentences\"] = df[\"SpeechToRate\"].apply(sentencize)\n",
    "    df_sents = sentencelist(df)\n",
    "    df_new = pd.DataFrame(pd.Series(df_sents), columns=[\"sentences\"])\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Group the transcript sentences into \"sections\" and write to a dataframe for input into ratings NN\n",
    "# Going from a dataframe with \"sentences\" to one with just \"sections,\" the size of tweets\n",
    "def sectionize_astweets(df):\n",
    "    current_section = \"\"\n",
    "    current_length = 0\n",
    "    sections = []\n",
    "    max_sent_len = df.sentences.str.len().max()\n",
    "    if max_sent_len >= 280:\n",
    "        for sentence in df[\"sentences\"]:\n",
    "            if current_length > 0 and current_length + len(sentence) >= 280:\n",
    "                sections.append(current_section)\n",
    "                current_section = sentence\n",
    "                current_length = len(sentence)\n",
    "            else:\n",
    "                current_section += sentence + \" \"\n",
    "                current_length += len(sentence) + 1\n",
    "    else:\n",
    "        for sentence in df[\"sentences\"]:\n",
    "            if current_length + len(sentence) >= 280:\n",
    "                sections.append(current_section)\n",
    "                current_section = sentence\n",
    "                current_length = len(sentence)\n",
    "            else:\n",
    "                current_section += sentence + \" \"\n",
    "                current_length += len(sentence) + 1\n",
    "    if current_length != 0:\n",
    "        sections.append(current_section)\n",
    "\n",
    "    df_new_sect = pd.DataFrame({\"sections\": sections})\n",
    "\n",
    "    return df_new_sect, max_sent_len"
   ],
   "metadata": {
    "id": "dcHeSOHuUuyQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682107924759,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     }
    }
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# This is for processing one \"SpeechToRate\" row from X_speech dataframe,\n",
    "# by breaking it up into 2 \"tweet-size\" sections and rating using\n",
    "# models built from tweet text\n",
    "def rate_speech_astweet(string):\n",
    "    dict_string = {\"SpeechToRate\": string}\n",
    "    df = pd.DataFrame(dict_string, index=[0])\n",
    "    df_new = sentencelistize(df)\n",
    "    df_new_sect, max_sent_len = sectionize_astweets(df_new)\n",
    "    sectioned_text = \"\"\n",
    "    # This is for outputting so can see where sectioned the sentences:\n",
    "    for ii in df_new_sect.index:\n",
    "        sectioned_text += df_new_sect.sections[ii] + \" !@@! \"\n",
    "    sectioned_text += str(max_sent_len)\n",
    "    # Predict ratings on tweet video and picture transcripts\n",
    "    nb_batches = math.ceil(len(df_new_sect) / BATCH_SIZE)\n",
    "    y_pred_m, y_pred_e, y_pred_n, y_pred_b = 0.0, 0.0, 0.0, 0.0\n",
    "    for i in range(nb_batches):\n",
    "        input_texts = df_new_sect[i * BATCH_SIZE : (i + 1) * BATCH_SIZE][\n",
    "            \"sections\"\n",
    "        ].tolist()\n",
    "        encoded = tokenizer(\n",
    "            input_texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=80,\n",
    "            return_tensors=\"tf\",\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "        pred_raw_m = misrep_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_m = np.argmax(pred_raw_m, axis=1)\n",
    "        for kl in range(len(y_preds_m)):\n",
    "            y_pred_m += decoded_dict_misrep[y_preds_m[kl]]\n",
    "        pred_raw_e = entitle_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_e = np.argmax(pred_raw_e, axis=1)\n",
    "        for kl in range(len(y_preds_e)):\n",
    "            y_pred_e += decoded_dict_entitle[y_preds_e[kl]]\n",
    "        pred_raw_n = neg_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_n = np.argmax(pred_raw_n, axis=1)\n",
    "        for kl in range(len(y_preds_n)):\n",
    "            y_pred_n += decoded_dict_neg[y_preds_n[kl]]\n",
    "        pred_raw_b = blame_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_b = np.argmax(pred_raw_b, axis=1)\n",
    "        for kl in range(len(y_preds_b)):\n",
    "            y_pred_b += decoded_dict_blame[y_preds_b[kl]]\n",
    "    subject_blob_speech, polar_blob_speech, polar_blob_pos_speech = 0.0, 0.0, 0.0\n",
    "    for mm in range(len(df_new)):\n",
    "        subject_blob_speech += getSubjectivity(df_new.sentences[mm])\n",
    "        if getPolarity(df_new.sentences[mm]) < 0:\n",
    "            polar_blob_speech += getPolarity(df_new.sentences[mm])\n",
    "        else:\n",
    "            polar_blob_pos_speech += getPolarity(df_new.sentences[mm])\n",
    "    return (\n",
    "        y_pred_m,\n",
    "        y_pred_e,\n",
    "        y_pred_n,\n",
    "        y_pred_b,\n",
    "        subject_blob_speech,\n",
    "        polar_blob_speech,\n",
    "        polar_blob_pos_speech,\n",
    "        sectioned_text,\n",
    "    )"
   ],
   "metadata": {
    "id": "RUjWzQpjeACg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682107924759,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     }
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682107924759,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "NWE2wwfNHKCJ"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://www.topcoder.com/thrive/articles/getting-started-with-textblob-for-sentiment-analysis#:~:text=When%20a%20sentence%20is%20passed,to%20personal%20opinions%20and%20judgments.\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682107924759,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "GGXNw11DbfPD"
   },
   "outputs": [],
   "source": [
    "# Replace @Usernames, abbreviations, #HashTags, and https://weblinks in tweets, translate to English, and Regex clean\n",
    "def preprocess_tweets(df):\n",
    "    df[\"preclean\"] = df[\"Tweet\"].apply(process_abbrevs)\n",
    "    df[\"preclean\"] = df[\"preclean\"].apply(process_hashtags)\n",
    "    df[\"preclean\"] = df[\"preclean\"].apply(process_urls)\n",
    "    df[\"preclean\"] = df[\"preclean\"].apply(process_users)\n",
    "    df = cleaner(df)\n",
    "    df = df.drop(columns=[\"Tweet\", \"preclean\"], axis=1)\n",
    "    df[\"polarity\"] = df[\"cleaned_tweet\"].apply(getPolarity)\n",
    "    df[\"subjectivity\"] = df[\"cleaned_tweet\"].apply(getSubjectivity)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682107924759,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "ehL4D3cjlp2_"
   },
   "outputs": [],
   "source": [
    "# Dictionaries to decode the tweet NN outputs back to actual ratings values in \"years\"\n",
    "decoded_dict_neg = {0: 0, 1: 2, 2: 3, 3: 5, 4: 7, 5: 11.1}\n",
    "decoded_dict_misrep = {0: 0, 1: 1, 2: 2, 3: 3.2}\n",
    "decoded_dict_entitle = {0: 0, 1: 2, 2: 5}\n",
    "decoded_dict_blame = {0: 0, 1: 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682107924759,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "QIlGeeaYZqtC"
   },
   "outputs": [],
   "source": [
    "def rate_tweets(df):\n",
    "    nb_batches = math.ceil(len(df) / BATCH_SIZE)\n",
    "    y_pred_m, y_pred_e, y_pred_n, y_pred_b = 0.0, 0.0, 0.0, 0.0\n",
    "    y_pred_ml, y_pred_el, y_pred_nl, y_pred_bl = [], [], [], []\n",
    "    for i in range(nb_batches):\n",
    "        input_texts = df[i * BATCH_SIZE : (i + 1) * BATCH_SIZE][\n",
    "            \"cleaned_tweet\"\n",
    "        ].tolist()\n",
    "        encoded = tokenizer(\n",
    "            input_texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=80,\n",
    "            return_tensors=\"tf\",\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "        pred_raw_m = misrep_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_m = np.argmax(pred_raw_m, axis=1)\n",
    "        for kl in range(len(y_preds_m)):\n",
    "            y_pred_m += decoded_dict_misrep[y_preds_m[kl]]\n",
    "            y_pred_ml.append(decoded_dict_misrep[y_preds_m[kl]])\n",
    "        pred_raw_e = entitle_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_e = np.argmax(pred_raw_e, axis=1)\n",
    "        for kl in range(len(y_preds_e)):\n",
    "            y_pred_e += decoded_dict_entitle[y_preds_e[kl]]\n",
    "            y_pred_el.append(decoded_dict_entitle[y_preds_e[kl]])\n",
    "        pred_raw_n = neg_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_n = np.argmax(pred_raw_n, axis=1)\n",
    "        for kl in range(len(y_preds_n)):\n",
    "            y_pred_n += decoded_dict_neg[y_preds_n[kl]]\n",
    "            y_pred_nl.append(decoded_dict_neg[y_preds_n[kl]])\n",
    "        pred_raw_b = blame_model.predict(\n",
    "            {\n",
    "                \"input_ids\": encoded[\"input_ids\"],\n",
    "                \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            },\n",
    "            verbose=0,\n",
    "        )\n",
    "        y_preds_b = np.argmax(pred_raw_b, axis=1)\n",
    "        for kl in range(len(y_preds_b)):\n",
    "            y_pred_b += decoded_dict_blame[y_preds_b[kl]]\n",
    "            y_pred_bl.append(decoded_dict_blame[y_preds_b[kl]])\n",
    "    return (\n",
    "        y_pred_ml,\n",
    "        y_pred_el,\n",
    "        y_pred_nl,\n",
    "        y_pred_bl,\n",
    "        y_pred_m,\n",
    "        y_pred_e,\n",
    "        y_pred_n,\n",
    "        y_pred_b,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1682107925022,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "GOVjc8ljPnpy"
   },
   "outputs": [],
   "source": [
    "def rate_embeds(df):\n",
    "    y_pred_m, y_pred_e, y_pred_n, y_pred_b = 0.0, 0.0, 0.0, 0.0\n",
    "    y_pred_ml, y_pred_el, y_pred_nl, y_pred_bl = [], [], [], []\n",
    "    for i in range(len(df)):\n",
    "        if df.cleaned_tweet[i] != \"\":\n",
    "            input_texts = df.cleaned_tweet[i]\n",
    "            encoded = tokenizer(\n",
    "                input_texts,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=80,\n",
    "                return_tensors=\"tf\",\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=True,\n",
    "                verbose=True,\n",
    "            )\n",
    "            pred_raw_m = misrep_model.predict(\n",
    "                {\n",
    "                    \"input_ids\": encoded[\"input_ids\"],\n",
    "                    \"attention_mask\": encoded[\"attention_mask\"],\n",
    "                },\n",
    "                verbose=0,\n",
    "            )\n",
    "            y_preds_m = np.argmax(pred_raw_m, axis=1)\n",
    "            for kl in range(len(y_preds_m)):\n",
    "                y_pred_m += decoded_dict_misrep[y_preds_m[kl]]\n",
    "                y_pred_ml.append(decoded_dict_misrep[y_preds_m[kl]])\n",
    "            pred_raw_e = entitle_model.predict(\n",
    "                {\n",
    "                    \"input_ids\": encoded[\"input_ids\"],\n",
    "                    \"attention_mask\": encoded[\"attention_mask\"],\n",
    "                },\n",
    "                verbose=0,\n",
    "            )\n",
    "            y_preds_e = np.argmax(pred_raw_e, axis=1)\n",
    "            for kl in range(len(y_preds_e)):\n",
    "                y_pred_e += decoded_dict_entitle[y_preds_e[kl]]\n",
    "                y_pred_el.append(decoded_dict_entitle[y_preds_e[kl]])\n",
    "            pred_raw_n = neg_model.predict(\n",
    "                {\n",
    "                    \"input_ids\": encoded[\"input_ids\"],\n",
    "                    \"attention_mask\": encoded[\"attention_mask\"],\n",
    "                },\n",
    "                verbose=0,\n",
    "            )\n",
    "            y_preds_n = np.argmax(pred_raw_n, axis=1)\n",
    "            for kl in range(len(y_preds_n)):\n",
    "                y_pred_n += decoded_dict_neg[y_preds_n[kl]]\n",
    "                y_pred_nl.append(decoded_dict_neg[y_preds_n[kl]])\n",
    "            pred_raw_b = blame_model.predict(\n",
    "                {\n",
    "                    \"input_ids\": encoded[\"input_ids\"],\n",
    "                    \"attention_mask\": encoded[\"attention_mask\"],\n",
    "                },\n",
    "                verbose=0,\n",
    "            )\n",
    "            y_preds_b = np.argmax(pred_raw_b, axis=1)\n",
    "            for kl in range(len(y_preds_b)):\n",
    "                y_pred_b += decoded_dict_blame[y_preds_b[kl]]\n",
    "                y_pred_bl.append(decoded_dict_blame[y_preds_b[kl]])\n",
    "        else:\n",
    "            y_pred_ml.append(0)\n",
    "            y_pred_el.append(0)\n",
    "            y_pred_nl.append(0)\n",
    "            y_pred_bl.append(0)\n",
    "    return (\n",
    "        y_pred_ml,\n",
    "        y_pred_el,\n",
    "        y_pred_nl,\n",
    "        y_pred_bl,\n",
    "        y_pred_m,\n",
    "        y_pred_e,\n",
    "        y_pred_n,\n",
    "        y_pred_b,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 114476,
     "status": "ok",
     "timestamp": 1682108039496,
     "user": {
      "displayName": "Sean Sweeney",
      "userId": "10921123275992044821"
     },
     "user_tz": 240
    },
    "id": "HLr-JT1Nw5V1"
   },
   "outputs": [],
   "source": [
    "# Load ConvBERT tweet ratings models (multiclass classification)\n",
    "entitle_model = load_model(\n",
    "    \"/content/drive/MyDrive/Colab_Notebooks/model_ConvBert_tweets_entitlev23PMmulticlass.h5\",\n",
    "    custom_objects={\"TFConvBertModel\": TFConvBertModel},\n",
    ")\n",
    "misrep_model = load_model(\n",
    "    \"/content/drive/MyDrive/Colab_Notebooks/model_ConvBert_tweets_misrepv17PMmulticlass.h5\",\n",
    "    custom_objects={\"TFConvBertModel\": TFConvBertModel},\n",
    ")\n",
    "neg_model = load_model(\n",
    "    \"/content/drive/MyDrive/Colab_Notebooks/model_ConvBert_tweets_negv17PMmulticlass.h5\",\n",
    "    custom_objects={\"TFConvBertModel\": TFConvBertModel},\n",
    ")\n",
    "blame_model = load_model(\n",
    "    \"/content/drive/MyDrive/Colab_Notebooks/model_ConvBert_tweets_blamev17PMmulticlass.h5\",\n",
    "    custom_objects={\"TFConvBertModel\": TFConvBertModel},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YgcXDDjYpoL"
   },
   "outputs": [],
   "source": [
    "v_num = \"17b\"\n",
    "BASE_MODEL = \"YituTech/conv-bert-base\"\n",
    "BATCH_SIZE = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Read in tweets for processing to ratings in units of lifetimes\n",
    "# Guide to variables: \"_t\" indicates for tweets, \"_em\" for embedded tweets and retweets, and\n",
    "# \"_s\" for \"speech,\" or transcripts of videos and photos\n",
    "polit_list = polit_list_old\n",
    "user_list_start = user_list_old\n",
    "# The following variables are for saving ratings for each member of polit_list:\n",
    "user_list = []\n",
    "polit_list_new = []\n",
    "misrep_pred_life_t, entitle_pred_life_t, neg_pred_life_t = [], [], []\n",
    "blame_pred_life_t = []\n",
    "misrep_pred_life_em, entitle_pred_life_em, neg_pred_life_em = [], [], []\n",
    "blame_pred_life_em = []\n",
    "misrep_pred_life_s, entitle_pred_life_s, neg_pred_life_s = [], [], []\n",
    "blame_pred_life_s = []\n",
    "misrep_pred_life_tot, entitle_pred_life_tot, neg_pred_life_tot = [], [], []\n",
    "blame_pred_life_tot = []\n",
    "subjectivity_blob_t, polarity_blob_t, polarity_blob_pos_t = [], [], []\n",
    "subjectivity_blob_em, polarity_blob_em, polarity_blob_pos_em = [], [], []\n",
    "subjectivity_blob_s, polarity_blob_s, polarity_blob_pos_s = [], [], []\n",
    "num_tweets, pics_count, vids_count = [], [], []\n",
    "k = -1\n",
    "for ki, j in enumerate(polit_list):\n",
    "    print(j)\n",
    "    try:\n",
    "        user = api.get_user(user_id=j)\n",
    "        j_name = user.screen_name\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        j_name = user_list_start[ki]\n",
    "    try:\n",
    "        k += 1\n",
    "        j_prev_name = user_list_start[k]\n",
    "        print(j_name)\n",
    "        tweets_df = pd.read_pickle(\n",
    "            \"/content/drive/MyDrive/Colab_Notebooks/PolitTweets/TwoMonthTweets\"\n",
    "            + end_date\n",
    "            + \"/\"\n",
    "            + j_prev_name\n",
    "            + \"TwoMonthTweets\"\n",
    "            + str(j)\n",
    "            + \"end\"\n",
    "            + end_date\n",
    "            + \".pkl\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if len(tweets_df) == 0:\n",
    "        continue\n",
    "    user_list.append(j_name)\n",
    "    polit_list_new.append(j)\n",
    "    num_tweets.append(len(tweets_df))\n",
    "    tweets_df = transcr_media(tweets_df, j)\n",
    "    tweets_df = transcr_clean(tweets_df)\n",
    "    tweets_df = filter_non_words(tweets_df)\n",
    "    tweets_df, pics_counts, vids_counts = sort_transcripts(tweets_df)\n",
    "    pics_count.append(pics_counts)\n",
    "    vids_count.append(vids_counts)\n",
    "    X_speech = pd.DataFrame().assign(SpeechToRate=tweets_df[\"SpeechToRate\"])\n",
    "    X_tweets = pd.DataFrame().assign(Tweet=tweets_df[\"TweetToRate\"])\n",
    "    X_embeds = pd.DataFrame().assign(Tweet=tweets_df[\"EmbedToRate\"])\n",
    "    # Rate each row of X_speech and add them up:\n",
    "    misrep_pred_speech, entitle_pred_speech, neg_pred_speech = 0.0, 0.0, 0.0\n",
    "    blame_pred_speech = 0.0\n",
    "    subject_blob_speech, polar_blob_speech, polar_blob_pos_speech = 0.0, 0.0, 0.0\n",
    "    # These variables are for saving ratings for each individual \"speech\" (transcribed videos and text from photos):\n",
    "    misrep_pred_s, entitle_pred_s, neg_pred_s = [], [], []\n",
    "    blame_pred_s, sectioned_text = [], []\n",
    "    subject_blob_s, polar_blob_s, polar_blob_pos_s = [], [], []\n",
    "    for m in X_speech.index:\n",
    "        misrep_pred_s.append(0.0)\n",
    "        entitle_pred_s.append(0.0)\n",
    "        neg_pred_s.append(0.0)\n",
    "        blame_pred_s.append(0.0)\n",
    "        subject_blob_s.append(0.0)\n",
    "        polar_blob_s.append(0.0)\n",
    "        polar_blob_pos_s.append(0.0)\n",
    "        sectioned_text.append(\"\")\n",
    "        if X_speech.SpeechToRate[m] != \"\":\n",
    "            (\n",
    "                misrep_pred_s[m],\n",
    "                entitle_pred_s[m],\n",
    "                neg_pred_s[m],\n",
    "                blame_pred_s[m],\n",
    "                subject_blob_s[m],\n",
    "                polar_blob_s[m],\n",
    "                polar_blob_pos_s[m],\n",
    "                sectioned_text[m],\n",
    "            ) = rate_speech_astweet(X_speech.SpeechToRate[m])\n",
    "            misrep_pred_speech += misrep_pred_s[m]\n",
    "            entitle_pred_speech += entitle_pred_s[m]\n",
    "            neg_pred_speech += neg_pred_s[m]\n",
    "            blame_pred_speech += blame_pred_s[m]\n",
    "            # don't differentiate retweet/embedded tweet in textblob ratings\n",
    "            subject_blob_speech += subject_blob_s[m]\n",
    "            polar_blob_speech += polar_blob_s[m]\n",
    "            polar_blob_pos_speech += polar_blob_pos_s[m]\n",
    "    for i in X_tweets.index:\n",
    "        X_tweets.Tweet[i] = str(X_tweets.Tweet[i])\n",
    "    X_tweets = preprocess_tweets(X_tweets)\n",
    "    for i in X_embeds.index:\n",
    "        X_embeds.Tweet[i] = str(X_embeds.Tweet[i])\n",
    "    X_embeds = preprocess_tweets(X_embeds)\n",
    "    tweets_df[\"cleaned_tweet\"] = X_tweets.cleaned_tweet.copy()\n",
    "    tweets_df[\"cleaned_embed\"] = X_embeds.cleaned_tweet.copy()\n",
    "    tweets_df.to_csv(\n",
    "        \"/content/drive/MyDrive/Colab_Notebooks/PolitTweets/TwoMonthTranscribes\"\n",
    "        + end_date\n",
    "        + \"/TwoMonthTranscribedv\"\n",
    "        + v_num\n",
    "        + j_name\n",
    "        + end_date\n",
    "        + \"user\"\n",
    "        + str(j)\n",
    "        + \"V2.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    tweets_df.to_pickle(\n",
    "        \"/content/drive/MyDrive/Colab_Notebooks/PolitTweets/TwoMonthTranscribes\"\n",
    "        + end_date\n",
    "        + \"/TwoMonthTranscribedv\"\n",
    "        + v_num\n",
    "        + j_name\n",
    "        + end_date\n",
    "        + \"user\"\n",
    "        + str(j)\n",
    "        + \"V2.pkl\"\n",
    "    )\n",
    "    (\n",
    "        misrep_pred_t,\n",
    "        entitle_pred_t,\n",
    "        neg_pred_t,\n",
    "        blame_pred_t,\n",
    "        misrep_pred_tweet,\n",
    "        entitle_pred_tweet,\n",
    "        neg_pred_tweet,\n",
    "        blame_pred_tweet,\n",
    "    ) = rate_tweets(X_tweets)\n",
    "    (\n",
    "        misrep_pred_em,\n",
    "        entitle_pred_em,\n",
    "        neg_pred_em,\n",
    "        blame_pred_em,\n",
    "        misrep_pred_embed,\n",
    "        entitle_pred_embed,\n",
    "        neg_pred_embed,\n",
    "        blame_pred_embed,\n",
    "    ) = rate_embeds(X_embeds)\n",
    "    misrep_pred_retweet, entitle_pred_retweet, neg_pred_retweet = 0.0, 0.0, 0.0\n",
    "    blame_pred_retweet = 0.0\n",
    "    for i in range(len(tweets_df)):\n",
    "        if tweets_df.Retweet[i] == 1:\n",
    "            misrep_pred_retweet += misrep_pred_t[i]\n",
    "            entitle_pred_retweet += entitle_pred_t[i]\n",
    "            neg_pred_retweet += neg_pred_t[i]\n",
    "            blame_pred_retweet += blame_pred_t[i]\n",
    "    # Put ratings of retweets into the embedded tweets rating, and take out of the regular tweet ratings\n",
    "    misrep_pred_tweet -= misrep_pred_retweet\n",
    "    misrep_pred_embed += misrep_pred_retweet\n",
    "    entitle_pred_tweet -= entitle_pred_retweet\n",
    "    entitle_pred_embed += entitle_pred_retweet\n",
    "    neg_pred_tweet -= neg_pred_retweet\n",
    "    neg_pred_embed += neg_pred_retweet\n",
    "    blame_pred_tweet -= blame_pred_retweet\n",
    "    blame_pred_embed += blame_pred_retweet\n",
    "    misrep_pred_life_t.append(round((misrep_pred_tweet) / 60.0, 2))\n",
    "    entitle_pred_life_t.append(round((entitle_pred_tweet) / 60.0, 2))\n",
    "    neg_pred_life_t.append(round((neg_pred_tweet) / 60.0, 2))\n",
    "    blame_pred_life_t.append(round((blame_pred_tweet) / 60.0, 2))\n",
    "    misrep_pred_life_em.append(round((misrep_pred_embed) / 60.0, 2))\n",
    "    entitle_pred_life_em.append(round((entitle_pred_embed) / 60.0, 2))\n",
    "    neg_pred_life_em.append(round((neg_pred_embed) / 60.0, 2))\n",
    "    blame_pred_life_em.append(round((blame_pred_embed) / 60.0, 2))\n",
    "    subject_blob_tweet, polar_blob_tweet, polar_blob_pos_tweet = 0.0, 0.0, 0.0\n",
    "    subject_blob_t, polar_blob_t, polar_blob_pos_t = [], [], []\n",
    "    subject_blob_embed, polar_blob_embed, polar_blob_pos_embed = 0.0, 0.0, 0.0\n",
    "    subject_blob_em, polar_blob_em, polar_blob_pos_em = [], [], []\n",
    "    for i in range(len(X_tweets)):\n",
    "        polar_blob_t.append(0.0)\n",
    "        polar_blob_em.append(0.0)\n",
    "        polar_blob_pos_t.append(0.0)\n",
    "        polar_blob_pos_em.append(0.0)\n",
    "        if X_tweets.polarity[i] < 0:\n",
    "            polar_blob_tweet += X_tweets.polarity[i]\n",
    "            polar_blob_t[i] = X_tweets.polarity[i]\n",
    "        else:\n",
    "            polar_blob_pos_tweet += X_tweets.polarity[i]\n",
    "            polar_blob_pos_t[i] = X_tweets.polarity[i]\n",
    "        if X_embeds.polarity[i] < 0:\n",
    "            polar_blob_embed += X_embeds.polarity[i]\n",
    "            polar_blob_em[i] = X_embeds.polarity[i]\n",
    "        else:\n",
    "            polar_blob_pos_embed += X_embeds.polarity[i]\n",
    "            polar_blob_pos_em[i] = X_embeds.polarity[i]\n",
    "    subject_blob_tweet = X_tweets.subjectivity.sum()\n",
    "    subject_blob_embed = X_embeds.subjectivity.sum()\n",
    "    subject_blob_t = X_tweets[\"subjectivity\"].values.tolist()\n",
    "    subject_blob_em = X_embeds[\"subjectivity\"].values.tolist()\n",
    "    # Save the ratings converted from units of years to lifetimes, for each member of polit_list\n",
    "    misrep_pred_life_s.append(misrep_pred_speech / 60.0)\n",
    "    entitle_pred_life_s.append(entitle_pred_speech / 60.0)\n",
    "    neg_pred_life_s.append(neg_pred_speech / 60.0)\n",
    "    blame_pred_life_s.append(blame_pred_speech / 60.0)\n",
    "    subjectivity_blob_t.append(subject_blob_tweet)\n",
    "    polarity_blob_t.append(polar_blob_tweet)\n",
    "    polarity_blob_pos_t.append(polar_blob_pos_tweet)\n",
    "    subjectivity_blob_em.append(subject_blob_tweet)\n",
    "    polarity_blob_em.append(polar_blob_tweet)\n",
    "    polarity_blob_pos_em.append(polar_blob_pos_tweet)\n",
    "    subjectivity_blob_s.append(subject_blob_speech)\n",
    "    polarity_blob_s.append(polar_blob_speech)\n",
    "    polarity_blob_pos_s.append(polar_blob_pos_speech)\n",
    "\n",
    "    tweets_rated = tweets_df.copy()\n",
    "    tweets_rated[\"SectionedSpeech\"] = sectioned_text\n",
    "    tweets_rated[\"Misrep_t\"] = misrep_pred_t\n",
    "    tweets_rated[\"Entitle_t\"] = entitle_pred_t\n",
    "    tweets_rated[\"Neg_t\"] = neg_pred_t\n",
    "    tweets_rated[\"Blame_t\"] = blame_pred_t\n",
    "    tweets_rated[\"Misrep_em\"] = misrep_pred_em\n",
    "    tweets_rated[\"Entitle_em\"] = entitle_pred_em\n",
    "    tweets_rated[\"Neg_em\"] = neg_pred_em\n",
    "    tweets_rated[\"Blame_em\"] = blame_pred_em\n",
    "    tweets_rated[\"Misrep_s\"] = misrep_pred_s\n",
    "    tweets_rated[\"Entitle_s\"] = entitle_pred_s\n",
    "    tweets_rated[\"Neg_s\"] = neg_pred_s\n",
    "    tweets_rated[\"Blame_s\"] = blame_pred_s\n",
    "    tweets_rated[\"Polar_blob_t\"] = polar_blob_t\n",
    "    tweets_rated[\"Polar_blob_pos_t\"] = polar_blob_pos_t\n",
    "    tweets_rated[\"Subject_blob_t\"] = subject_blob_t\n",
    "    tweets_rated[\"Polar_blob_em\"] = polar_blob_em\n",
    "    tweets_rated[\"Polar_blob_pos_em\"] = polar_blob_pos_em\n",
    "    tweets_rated[\"Subject_blob_em\"] = subject_blob_em\n",
    "    tweets_rated[\"Polar_blob_s\"] = polar_blob_s\n",
    "    tweets_rated[\"Polar_blob_pos_s\"] = polar_blob_pos_s\n",
    "    tweets_rated[\"Subject_blob_s\"] = subject_blob_s\n",
    "    tweets_rated.to_csv(\n",
    "        \"/content/drive/MyDrive/Colab_Notebooks/PolitTweets/Classified/TwoMonthRatingsSpeechasTweetV\"\n",
    "        + v_num\n",
    "        + j_name\n",
    "        + end_date\n",
    "        + \"user\"\n",
    "        + str(j)\n",
    "        + \"V2.csv\",\n",
    "        index=False,\n",
    "    )"
   ],
   "metadata": {
    "id": "HgNjAe9_0arl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbVb4MnNdJS1"
   },
   "outputs": [],
   "source": [
    "# Tabulate by politician, rank by negativity rating, export to file\n",
    "ratings_df = pd.DataFrame()\n",
    "ratings_df[\"Username\"] = user_list\n",
    "ratings_df[\"NumTweets\"] = num_tweets\n",
    "# pics_count is number of photos with text in them\n",
    "# vids_count is number of videos with transcribed text\n",
    "ratings_df[\"NumPics\"] = pics_count\n",
    "ratings_df[\"NumVids\"] = vids_count\n",
    "ratings_df[\"Misrep_t\"] = misrep_pred_life_t\n",
    "ratings_df[\"Entitle_t\"] = entitle_pred_life_t\n",
    "ratings_df[\"Neg_t\"] = neg_pred_life_t\n",
    "ratings_df[\"Blame_t\"] = blame_pred_life_t\n",
    "ratings_df[\"Misrep_em\"] = misrep_pred_life_em\n",
    "ratings_df[\"Entitle_em\"] = entitle_pred_life_em\n",
    "ratings_df[\"Neg_em\"] = neg_pred_life_em\n",
    "ratings_df[\"Blame_em\"] = blame_pred_life_em\n",
    "ratings_df[\"Misrep_s\"] = misrep_pred_life_s\n",
    "ratings_df[\"Entitle_s\"] = entitle_pred_life_s\n",
    "ratings_df[\"Neg_s\"] = neg_pred_life_s\n",
    "ratings_df[\"Blame_s\"] = blame_pred_life_s\n",
    "ratings_df[\"BlobPolar_t\"] = polarity_blob_t\n",
    "ratings_df[\"BlobPolarPos_t\"] = polarity_blob_pos_t\n",
    "ratings_df[\"BlobSubject_t\"] = subjectivity_blob_t\n",
    "ratings_df[\"BlobPolar_em\"] = polarity_blob_em\n",
    "ratings_df[\"BlobPolarPos_em\"] = polarity_blob_pos_em\n",
    "ratings_df[\"BlobSubject_em\"] = subjectivity_blob_em\n",
    "ratings_df[\"BlobPolar_s\"] = polarity_blob_s\n",
    "ratings_df[\"BlobPolarPos_s\"] = polarity_blob_pos_s\n",
    "ratings_df[\"BlobSubject_s\"] = subjectivity_blob_s\n",
    "ratings_df[\"User ID\"] = polit_list_new\n",
    "neg_pred_life_all = []\n",
    "for i in range(len(neg_pred_life_t)):\n",
    "    neg_pred_life_all.append(\n",
    "        neg_pred_life_t[i] + neg_pred_life_em[i] + neg_pred_life_s[i]\n",
    "    )\n",
    "ratings_df[\"Neg\"] = neg_pred_life_all\n",
    "ratings_df.sort_values(\n",
    "    by=[\"Neg\"], axis=0, ascending=False, inplace=True, ignore_index=True\n",
    ")\n",
    "rank = []\n",
    "for i in range(len(ratings_df)):\n",
    "    rank.append(i + 1)\n",
    "ratings_df[\"Rank\"] = rank\n",
    "ratings_df.to_csv(\n",
    "    \"/content/drive/MyDrive/Colab_Notebooks/PolitTweets/TwoMonthResults\"\n",
    "    + end_date\n",
    "    + \"/TwoMonthRatingsRawV\"\n",
    "    + v_num\n",
    "    + end_date\n",
    "    + \"Day\"\n",
    "    + str(day_num)\n",
    "    + \".csv\",\n",
    "    index=False,\n",
    ")\n",
    "ratings_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1rr_L3x7POU45OzfDeik_ovJ3ifd_p3l0",
     "timestamp": 1682006631297
    },
    {
     "file_id": "1wMYuMXfZ8p1M0lUGAt7o_fIn-pUgiHS_",
     "timestamp": 1681301743104
    },
    {
     "file_id": "1nRIf6wEOrLx7dnvHJAO5PD8WuXOkTYSg",
     "timestamp": 1681046726873
    },
    {
     "file_id": "1DqVLQBaC2XKvF5fi7jKIVRp_FdljLUvr",
     "timestamp": 1680272776607
    },
    {
     "file_id": "1wlbslbsgPgQZtVjztyrwzdBZFGeZZDij",
     "timestamp": 1680104812605
    },
    {
     "file_id": "14-zIQnBPNsetl1r7Zg9zw9tDt68Z8zqC",
     "timestamp": 1679669310367
    },
    {
     "file_id": "1-yiwAHnAzYyFGxTX1d6nfyOP0lV-XTPY",
     "timestamp": 1679073439874
    },
    {
     "file_id": "1P-ajqKOhWhT0nEBJNzef76qlPKhWANU3",
     "timestamp": 1678391379837
    },
    {
     "file_id": "1lcx1IYkkHZh3594JO1VGstJre9ib8hXk",
     "timestamp": 1675783117967
    },
    {
     "file_id": "1LAsnmz6wkKE70JrTMtNE-RChX5KY8kOI",
     "timestamp": 1674772589363
    },
    {
     "file_id": "1ZY0dcrcORBkg9gccjbq24i4k2zFbvF1e",
     "timestamp": 1674269632083
    },
    {
     "file_id": "1LulF6d49BeIRtReeoFQTnhlYaxBi85TU",
     "timestamp": 1674091691652
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}